{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "import re\n",
    "import numpy as np\n",
    "import glob\n",
    "import tricky_tables\n",
    "\n",
    "COLUMNS = [\"Emission Source\",\"Source Name\", \"Air Contaminant Name\", \"Emission Rate lbs/hr\", \"Emission Rate tons/year\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TABlE_WRITE_LOCATION = '../data/extracted_tables'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A series of text files to keep track of which PDFs have been processed. \n",
    "\n",
    "Processed PDFs fall into 3 categories \n",
    "* unknown -- parser failed\n",
    "* easy  -- `pdf_plumber.extract_tables()` successfully extracted table\n",
    "* tricky  -- customer parser in `tricky_table.py` sucessfuly extracted table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_file_location  = '../data/MAERT'\n",
    "\n",
    "files = glob.glob(f\"{base_file_location}/*.pdf\")\n",
    "\n",
    "with open('text_files/tables_to_extract.txt', 'w') as f:\n",
    "    for line in files:\n",
    "        f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_file = 'text_files/tables_to_extract.txt'\n",
    "easy_file = 'text_files/easy_tables.txt'\n",
    "tricky_file = 'text_files/tricky_tables.txt'\n",
    "unkown_file = 'text_files/unknown_tables.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_to_extract_txt = open(table_file, \"r\") \n",
    "tables_to_extract = tables_to_extract_txt.read().split(\"\\n\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_location in tables_to_extract:\n",
    "    file_name_raw = data_location.split(\"/\")[-1]\n",
    "    print(f\"Extracting from {file_name}\")\n",
    "    try:\n",
    "\n",
    "        pdf = pdfplumber.open(file_name)\n",
    "\n",
    "        total_pages = []\n",
    "        last_page_passed = False\n",
    "        tricky_table = False\n",
    "        no_data_left = False\n",
    "\n",
    "        for page in pdf.pages:\n",
    "            if last_page_passed == True:\n",
    "                    break\n",
    "            else:\n",
    "                \"\"\"\n",
    "                ┌─────────────────── ･ ｡ﾟ★: *.☪ .* :☆ﾟ. ───────────────────────┐\n",
    "\n",
    "                Case 1: Table is not nicely extractable\n",
    "                1. Extract table from the text with the stopping point\n",
    "                << (1)   Emission point identification >>\n",
    "                2. Parse through each line based on several cases.\n",
    "                └─────────────────── ･ ｡ﾟ★: *.☪ .* :☆ﾟ. ───────────────────────┘\n",
    "                \"\"\"\n",
    "                print(f\"Extracting from {page.page_number}\")\n",
    "\n",
    "                text = page.extract_text(keep_blank_chars=True)\n",
    "                blank_chars_false = page.extract_text()\n",
    "\n",
    "                if page.extract_table() == None:\n",
    "                    tricky_table = True\n",
    "\n",
    "                    try:\n",
    "                        core_pat = re.compile(r\"TPY[\\-\\s]+(.*)\\n\\s+\", re.DOTALL)\n",
    "                        core = re.search(core_pat, text).group(1)\n",
    "                    except Exception as e:\n",
    "                        core = text\n",
    "\n",
    "                    lines = core.split(\"\\n\")\n",
    "                    \n",
    "                    if \"point identification\" in blank_chars_false:\n",
    "                        if blank_chars_false.index(\"(1) Emission point identification\") < 100:\n",
    "                            no_data_left = True\n",
    "                        try:\n",
    "                            ending_line = [x for x in lines if \"pointidentification\" in x.replace(\" \",\"\") ]\n",
    "                            idx = lines.index(ending_line[0])\n",
    "                            lines = lines[:idx]\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                        last_page_passed = True\n",
    "\n",
    "                    # function from helper script\n",
    "                    if not no_data_left:\n",
    "                        df_page = tricky_tables.extract_table_custom(lines,COLUMNS)\n",
    "                        total_pages.append(df_page)\n",
    "\n",
    "                else:\n",
    "                    \n",
    "                    df_extracted_table = pd.DataFrame(pdf.pages[0].extract_table()[2:],columns=COLUMNS)\n",
    "                    df_extracted_table['Emission Source'] = df_extracted_table['Emission Source'].ffill()\n",
    "                    df_extracted_table['Source Name'] = df_extracted_table['Source Name'].ffill()\n",
    "\n",
    "                    total_pages.append(df_extracted_table)\n",
    "\n",
    "                    if \"point identification\" in blank_chars_false:\n",
    "                        last_page_passed = True\n",
    "\n",
    "        \"\"\"\n",
    "        ┌─────────────────── ･ ｡ﾟ★: *.☪ .* :☆ﾟ. ───────────────────────┐\n",
    "\n",
    "        Tackle known formatting errors\n",
    "        └─────────────────── ･ ｡ﾟ★: *.☪ .* :☆ﾟ. ───────────────────────┘\n",
    "        \"\"\"\n",
    "\n",
    "        if tricky_table:\n",
    "            df_pages = pd.concat(total_pages).dropna(axis = 0, how = 'all').reset_index(drop=True)\n",
    "            df_pages_cleaned = tricky_tables.clean_up_tricky_table(df_pages)\n",
    "        else:\n",
    "            df_pages = pd.concat(total_pages).reset_index(drop=True)\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        ┌─────────────────── ･ ｡ﾟ★: *.☪ .* :☆ﾟ. ───────────────────────┐\n",
    "\n",
    "        Add entity, permit number, publish date and write to CSV\n",
    "        └─────────────────── ･ ｡ﾟ★: *.☪ .* :☆ﾟ. ───────────────────────┘\n",
    "        \"\"\"\n",
    "\n",
    "        sp = file_name.split(\"_\")\n",
    "        entity = sp[0]\n",
    "        permit_number = sp[1]\n",
    "        publish_date = sp[2]\n",
    "\n",
    "        df_pages_cleaned['entity'] = entity\n",
    "        df_pages_cleaned['permit_number'] = permit_number\n",
    "        df_pages_cleaned['publish_date'] = publish_date\n",
    "\n",
    "        # lil hack to deal with table extraction method picking up the columns\n",
    "        df_pages_cleaned_dropped = df_pages_cleaned[~(df_pages_cleaned['Emission Source'] == 'Emission')]\n",
    "\n",
    "        df_pages_cleaned_dropped.to_csv(f\"{TABlE_WRITE_LOCATION}/{file_name.replace('pdf','csv')}\",index=False)\n",
    "\n",
    "        \"\"\"\n",
    "        ┌─────────────────── ･ ｡ﾟ★: *.☪ .* :☆ﾟ. ───────────────────────┐\n",
    "\n",
    "        Record metadata for extracted table\n",
    "        └─────────────────── ･ ｡ﾟ★: *.☪ .* :☆ﾟ. ───────────────────────┘\n",
    "        \"\"\"\n",
    "\n",
    "        if tricky_table:\n",
    "            fn = tricky_file\n",
    "        else:\n",
    "            fn = easy_file\n",
    "        f = open(fn, \"a\")\n",
    "        f.write(f\"{file_name}\\n\")\n",
    "        f.close()\n",
    "\n",
    "        print(\"Extracted succesfully!!!\")\n",
    "        print(\"-\"*10)\n",
    "        \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        f = open(unkown_file, \"a\")\n",
    "        f.write(f\"{file_name}\\n\")\n",
    "        f.close()\n",
    "        print(\"Extracted failed!!!\")\n",
    "        print(\"-\"*10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
